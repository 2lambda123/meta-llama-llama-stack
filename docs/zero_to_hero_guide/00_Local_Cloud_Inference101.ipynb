{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ed972d",
   "metadata": {},
   "source": [
    "# Switching between Local and Cloud Model with Llama Stack\n",
    "\n",
    "This guide provides a streamlined setup to switch between local and cloud clients for text generation with Llama Stackâ€™s `chat_completion` API. This setup enables automatic fallback to a cloud instance if the local client is unavailable.\n",
    "\n",
    "### Pre-requisite\n",
    "Before you begin, please ensure Llama Stack is installed and the distribution is set up by following the [Getting Started Guide](https://llama-stack.readthedocs.io/en/latest/). You will need to run two distributions, a local and a cloud distribution, for this demo to work.\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df89cff7",
   "metadata": {},
   "source": [
    "#### 1. Set Up Local and Cloud Clients\n",
    "\n",
    "Initialize both clients, specifying the `base_url` for each instance. In this case, we have the local distribution running on `http://localhost:5000` and the cloud distribution running on `http://localhost:5001`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f868dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# Configure local and cloud clients\n",
    "local_client = LlamaStackClient(base_url='http://{HOST}:{LOCAL_PORT}')\n",
    "cloud_client = LlamaStackClient(base_url='http://{HOST}:{CLOUD_PORT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894689c1",
   "metadata": {},
   "source": [
    "#### 2. Client Selection with Fallback\n",
    "\n",
    "The `select_client` function checks if the local client is available using a lightweight `/health` check. If the local client is unavailable, it automatically switches to the cloud client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0c8277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from termcolor import cprint\n",
    "\n",
    "async def select_client() -> LlamaStackClient:\n",
    "    \"\"\"Use local client if available; otherwise, switch to cloud client.\"\"\"\n",
    "    try:\n",
    "        async with httpx.AsyncClient() as http_client:\n",
    "            response = await http_client.get(f'{local_client.base_url}/health')\n",
    "            if response.status_code == 200:\n",
    "                cprint('Using local client.', 'yellow')\n",
    "                return local_client\n",
    "    except httpx.RequestError:\n",
    "        pass\n",
    "    cprint('Local client unavailable. Switching to cloud client.', 'yellow')\n",
    "    return cloud_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccfe66f",
   "metadata": {},
   "source": [
    "#### 3. Generate a Response\n",
    "\n",
    "After selecting the client, you can generate text using `chat_completion`. This example sends a sample prompt to the model and prints the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e19cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.types import UserMessage\n",
    "\n",
    "async def get_llama_response(stream: bool = True):\n",
    "    client = await select_client()  # Selects the available client\n",
    "    message = UserMessage(content='hello world, write me a 2 sentence poem about the moon', role='user')\n",
    "    cprint(f'User> {message.content}', 'green')\n",
    "\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=[message],\n",
    "        model='Llama3.2-11B-Vision-Instruct',\n",
    "        stream=stream,\n",
    "    )\n",
    "\n",
    "    if not stream:\n",
    "        cprint(f'> Response: {response}', 'cyan')\n",
    "    else:\n",
    "        # Stream tokens progressively\n",
    "        async for log in EventLogger().log(response):\n",
    "            log.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf5e57",
   "metadata": {},
   "source": [
    "#### 4. Run the Asynchronous Response Generation\n",
    "\n",
    "Use `asyncio.run()` to execute `get_llama_response` in an asynchronous event loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10f487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Initiate the response generation process\n",
    "asyncio.run(get_llama_response())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa9a09",
   "metadata": {},
   "source": [
    "### Complete code\n",
    "Summing it up, here's the complete code for local-cloud model implementation with Llama Stack:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd74ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import httpx\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "from llama_stack_client.types import UserMessage\n",
    "from termcolor import cprint\n",
    "\n",
    "local_client = LlamaStackClient(base_url='http://{HOST}:{LOCAL_PORT}')\n",
    "cloud_client = LlamaStackClient(base_url='http://{HOST}:{CLOUD_PORT}')\n",
    "\n",
    "async def select_client() -> LlamaStackClient:\n",
    "    try:\n",
    "        async with httpx.AsyncClient() as http_client:\n",
    "            response = await http_client.get(f'{local_client.base_url}/health')\n",
    "            if response.status_code == 200:\n",
    "                cprint('Using local client.', 'yellow')\n",
    "                return local_client\n",
    "    except httpx.RequestError:\n",
    "        pass\n",
    "    cprint('Local client unavailable. Switching to cloud client.', 'yellow')\n",
    "    return cloud_client\n",
    "\n",
    "async def get_llama_response(stream: bool = True):\n",
    "    client = await select_client()\n",
    "    message = UserMessage(\n",
    "        content='hello world, write me a 2 sentence poem about the moon', role='user'\n",
    "    )\n",
    "    cprint(f'User> {message.content}', 'green')\n",
    "\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=[message],\n",
    "        model='Llama3.2-11B-Vision-Instruct',\n",
    "        stream=stream,\n",
    "    )\n",
    "\n",
    "    if not stream:\n",
    "        cprint(f'> Response: {response}', 'cyan')\n",
    "    else:\n",
    "        async for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "\n",
    "asyncio.run(get_llama_response())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3a3ffa",
   "metadata": {},
   "source": [
    "Thanks for checking out this notebook! \n",
    "\n",
    "The next one will be a guide on [Prompt Engineering](./01_Prompt_Engineering101.ipynb), please continue learning!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
