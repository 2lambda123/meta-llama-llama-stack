{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/xiyan/Desktop/meta-llama/llama-stack\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: blobfile in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (3.0.0)\n",
      "Requirement already satisfied: fire in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (0.7.0)\n",
      "Requirement already satisfied: httpx in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (0.26.2)\n",
      "Requirement already satisfied: llama-models>=0.0.63 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (0.0.64rc1)\n",
      "Requirement already satisfied: llama-stack-client>=0.0.63 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (0.0.63)\n",
      "Requirement already satisfied: prompt-toolkit in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (3.0.48)\n",
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (1.0.1)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (2.9.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (2.32.3)\n",
      "Requirement already satisfied: rich in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (13.9.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (75.1.0)\n",
      "Requirement already satisfied: termcolor in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama_stack==0.0.63) (2.5.0)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-models>=0.0.63->llama_stack==0.0.63) (6.0.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-models>=0.0.63->llama_stack==0.0.63) (3.1.4)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-models>=0.0.63->llama_stack==0.0.63) (0.7.0)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-models>=0.0.63->llama_stack==0.0.63) (10.4.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-stack-client>=0.0.63->llama_stack==0.0.63) (4.6.2.post1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-stack-client>=0.0.63->llama_stack==0.0.63) (8.1.7)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-stack-client>=0.0.63->llama_stack==0.0.63) (1.9.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-stack-client>=0.0.63->llama_stack==0.0.63) (2.2.3)\n",
      "Requirement already satisfied: pyaml in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-stack-client>=0.0.63->llama_stack==0.0.63) (24.9.0)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-stack-client>=0.0.63->llama_stack==0.0.63) (1.3.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-stack-client>=0.0.63->llama_stack==0.0.63) (4.66.6)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from llama-stack-client>=0.0.63->llama_stack==0.0.63) (4.12.2)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from httpx->llama_stack==0.0.63) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from httpx->llama_stack==0.0.63) (1.0.6)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from httpx->llama_stack==0.0.63) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama_stack==0.0.63) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from pydantic>=2->llama_stack==0.0.63) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from pydantic>=2->llama_stack==0.0.63) (2.23.4)\n",
      "Requirement already satisfied: pycryptodomex>=3.8 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from blobfile->llama_stack==0.0.63) (3.21.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from blobfile->llama_stack==0.0.63) (2.2.3)\n",
      "Requirement already satisfied: lxml>=4.9 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from blobfile->llama_stack==0.0.63) (5.3.0)\n",
      "Requirement already satisfied: filelock>=3.0 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from blobfile->llama_stack==0.0.63) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from huggingface-hub->llama_stack==0.0.63) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from huggingface-hub->llama_stack==0.0.63) (24.1)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from prompt-toolkit->llama_stack==0.0.63) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from requests->llama_stack==0.0.63) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from rich->llama_stack==0.0.63) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from rich->llama_stack==0.0.63) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->llama_stack==0.0.63) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from jinja2->llama-models>=0.0.63->llama_stack==0.0.63) (2.1.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from pandas->llama-stack-client>=0.0.63->llama_stack==0.0.63) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from pandas->llama-stack-client>=0.0.63->llama_stack==0.0.63) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from pandas->llama-stack-client>=0.0.63->llama_stack==0.0.63) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from pandas->llama-stack-client>=0.0.63->llama_stack==0.0.63) (2024.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from tiktoken->llama-models>=0.0.63->llama_stack==0.0.63) (2024.9.11)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/master/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-stack-client>=0.0.63->llama_stack==0.0.63) (1.16.0)\n",
      "Building wheels for collected packages: llama_stack\n",
      "  Building editable for llama_stack (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama_stack: filename=llama_stack-0.0.63-0.editable-py3-none-any.whl size=8163 sha256=7ed2b324a367fdab6bd1119de51b2034bc08cf5a3aed2fe287007f0d2322dd5f\n",
      "  Stored in directory: /private/var/folders/rb/qv8vwgyj6yjd3t4pwsy9t0rm0000gn/T/pip-ephem-wheel-cache-iapx5ll8/wheels/e2/c6/4f/65df27da38dce470706f040fd5b53ac7b389c9c306795e3c4c\n",
      "Successfully built llama_stack\n",
      "Installing collected packages: llama_stack\n",
      "  Attempting uninstall: llama_stack\n",
      "    Found existing installation: llama_stack 0.0.63\n",
      "    Uninstalling llama_stack-0.0.63:\n",
      "      Successfully uninstalled llama_stack-0.0.63\n",
      "Successfully installed llama_stack-0.0.63\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWarning: `bwrap` is not available. Code interpreter tool will not work correctly.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/master/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using config <span style=\"color: #000080; text-decoration-color: #000080\">fireworks</span>:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using config \u001b[34mfireworks\u001b[0m:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- memory\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "conda_env: fireworks\n",
       "datasets: <span style=\"font-weight: bold\">[]</span>\n",
       "docker_image: null\n",
       "eval_tasks: <span style=\"font-weight: bold\">[]</span>\n",
       "image_name: fireworks\n",
       "memory_banks: <span style=\"font-weight: bold\">[]</span>\n",
       "metadata_store:\n",
       "  db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/xiyan/.llama/distributions/fireworks/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">registry.db</span>\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span>-8B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p1-8b-instruct\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span>-70B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p1-70b-instruct\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.1</span>-405B-Instruct-FP8\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p1-405b-instruct\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-1B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p2-1b-instruct\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-3B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p2-3b-instruct\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-11B-Vision-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p2-11b-vision-instruct\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span>-90B-Vision-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.3</span>-70B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p3-70b-instruct\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-Guard-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-8B\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-guard-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-8b\n",
       "- metadata: <span style=\"font-weight: bold\">{}</span>\n",
       "  model_id: meta-llama/Llama-Guard-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-11B-Vision\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-guard-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-11b-vision\n",
       "- metadata:\n",
       "    embedding_dimension: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">384</span>\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: sentence-transformers\n",
       "  provider_model_id: null\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/xiyan/.llama/distributions/fireworks/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">agents_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      url: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.fireworks.ai/inference/v1</span>\n",
       "    provider_id: fireworks\n",
       "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::f</span>ireworks\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: sentence-transformers\n",
       "    provider_type: inline::sentence-transformers\n",
       "  memory:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/xiyan/.llama/distributions/fireworks/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">faiss_store.db</span>\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::fa</span>iss\n",
       "  safety:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: basic\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::ba</span>sic\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>raintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: llama-stack\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: <span style=\"color: #800080; text-decoration-color: #800080\">/Users/xiyan/.llama/distributions/accounts/fireworks/models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">trace_store.db</span>\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::b</span>rave-search\n",
       "  - config:\n",
       "      api_key: <span style=\"color: #008000; text-decoration-color: #008000\">'********'</span>\n",
       "      max_results: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">e::c</span>ode-interpreter\n",
       "  - config: <span style=\"font-weight: bold\">{}</span>\n",
       "    provider_id: memory-runtime\n",
       "    provider_type: inline::memory-runtime\n",
       "scoring_fns: <span style=\"font-weight: bold\">[]</span>\n",
       "shields:\n",
       "- params: null\n",
       "  provider_id: null\n",
       "  provider_shield_id: null\n",
       "  shield_id: meta-llama/Llama-Guard-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>-8B\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: memory-runtime\n",
       "  toolgroup_id: builtin::memory\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "version: <span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "apis:\n",
       "- agents\n",
       "- datasetio\n",
       "- eval\n",
       "- inference\n",
       "- memory\n",
       "- safety\n",
       "- scoring\n",
       "- telemetry\n",
       "- tool_runtime\n",
       "conda_env: fireworks\n",
       "datasets: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "docker_image: null\n",
       "eval_tasks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "image_name: fireworks\n",
       "memory_banks: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "metadata_store:\n",
       "  db_path: \u001b[35m/Users/xiyan/.llama/distributions/fireworks/\u001b[0m\u001b[95mregistry.db\u001b[0m\n",
       "  namespace: null\n",
       "  type: sqlite\n",
       "models:\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.1\u001b[0m-8B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p1-8b-instruct\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.1\u001b[0m-70B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p1-70b-instruct\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.1\u001b[0m-405B-Instruct-FP8\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p1-405b-instruct\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-1B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p2-1b-instruct\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-3B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p2-3b-instruct\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-11B-Vision-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p2-11b-vision-instruct\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.2\u001b[0m-90B-Vision-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-\u001b[1;36m3.3\u001b[0m-70B-Instruct\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-v3p3-70b-instruct\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-Guard-\u001b[1;36m3\u001b[0m-8B\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-guard-\u001b[1;36m3\u001b[0m-8b\n",
       "- metadata: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "  model_id: meta-llama/Llama-Guard-\u001b[1;36m3\u001b[0m-11B-Vision\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - llm\n",
       "  provider_id: fireworks\n",
       "  provider_model_id: accounts/fireworks/models/llama-guard-\u001b[1;36m3\u001b[0m-11b-vision\n",
       "- metadata:\n",
       "    embedding_dimension: \u001b[1;36m384\u001b[0m\n",
       "  model_id: all-MiniLM-L6-v2\n",
       "  model_type: !!python/object/apply:llama_stack.apis.models.models.ModelType\n",
       "  - embedding\n",
       "  provider_id: sentence-transformers\n",
       "  provider_model_id: null\n",
       "providers:\n",
       "  agents:\n",
       "  - config:\n",
       "      persistence_store:\n",
       "        db_path: \u001b[35m/Users/xiyan/.llama/distributions/fireworks/\u001b[0m\u001b[95magents_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  datasetio:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: huggingface\n",
       "    provider_type: remote::huggingface\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: localfs\n",
       "    provider_type: inline::localfs\n",
       "  eval:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  inference:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      url: \u001b[4;94mhttps://api.fireworks.ai/inference/v1\u001b[0m\n",
       "    provider_id: fireworks\n",
       "    provider_type: remot\u001b[1;92me::f\u001b[0mireworks\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: sentence-transformers\n",
       "    provider_type: inline::sentence-transformers\n",
       "  memory:\n",
       "  - config:\n",
       "      kvstore:\n",
       "        db_path: \u001b[35m/Users/xiyan/.llama/distributions/fireworks/\u001b[0m\u001b[95mfaiss_store.db\u001b[0m\n",
       "        namespace: null\n",
       "        type: sqlite\n",
       "    provider_id: faiss\n",
       "    provider_type: inlin\u001b[1;92me::fa\u001b[0miss\n",
       "  safety:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: llama-guard\n",
       "    provider_type: inline::llama-guard\n",
       "  scoring:\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: basic\n",
       "    provider_type: inlin\u001b[1;92me::ba\u001b[0msic\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: llm-as-judge\n",
       "    provider_type: inline::llm-as-judge\n",
       "  - config:\n",
       "      openai_api_key: \u001b[32m'********'\u001b[0m\n",
       "    provider_id: braintrust\n",
       "    provider_type: inlin\u001b[1;92me::b\u001b[0mraintrust\n",
       "  telemetry:\n",
       "  - config:\n",
       "      service_name: llama-stack\n",
       "      sinks: sqlite\n",
       "      sqlite_db_path: \u001b[35m/Users/xiyan/.llama/distributions/accounts/fireworks/models/\u001b[0m\u001b[95mtrace_store.db\u001b[0m\n",
       "    provider_id: meta-reference\n",
       "    provider_type: inline::meta-reference\n",
       "  tool_runtime:\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: brave-search\n",
       "    provider_type: remot\u001b[1;92me::b\u001b[0mrave-search\n",
       "  - config:\n",
       "      api_key: \u001b[32m'********'\u001b[0m\n",
       "      max_results: \u001b[1;36m3\u001b[0m\n",
       "    provider_id: tavily-search\n",
       "    provider_type: remote::tavily-search\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: code-interpreter\n",
       "    provider_type: inlin\u001b[1;92me::c\u001b[0mode-interpreter\n",
       "  - config: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    provider_id: memory-runtime\n",
       "    provider_type: inline::memory-runtime\n",
       "scoring_fns: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "shields:\n",
       "- params: null\n",
       "  provider_id: null\n",
       "  provider_shield_id: null\n",
       "  shield_id: meta-llama/Llama-Guard-\u001b[1;36m3\u001b[0m-8B\n",
       "tool_groups:\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: tavily-search\n",
       "  toolgroup_id: builtin::websearch\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: memory-runtime\n",
       "  toolgroup_id: builtin::memory\n",
       "- args: null\n",
       "  mcp_endpoint: null\n",
       "  provider_id: code-interpreter\n",
       "  toolgroup_id: builtin::code_interpreter\n",
       "version: \u001b[32m'2'\u001b[0m\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from llama_stack.distribution.library_client import LlamaStackAsLibraryClient\n",
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "\n",
    "# os.environ[\"INFERENCE_MODEL\"] = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# os.environ[\"OLLAMA_INFERENCE_MODEL\"] = \"llama3.1:8b-instruct-fp16\"\n",
    "# client = LlamaStackClient(\"\")\n",
    "# client = LlamaStackClient(\n",
    "#     base_url=f\"http://localhost:5000\",\n",
    "# )\n",
    "client = LlamaStackAsLibraryClient(\"fireworks\", skip_logger_removal=True)\n",
    "_ = client.initialize()\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# response = client.inference.chat_completion(\n",
    "#     model_id=model_id,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"Write a two-sentence poem about llama.\"},\n",
    "#     ],\n",
    "#     stream=True,\n",
    "# )\n",
    "# for x in response:\n",
    "#     print(x)\n",
    "# for log in EventLogger().log(response):\n",
    "#     log.print()\n",
    "\n",
    "\n",
    "# response2 = client.inference.chat_completion(\n",
    "#     model_id=model_id,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": \"What's up?\"},\n",
    "#     ],\n",
    "#     stream=True,\n",
    "# )\n",
    "# for log in EventLogger().log(response2):\n",
    "#     log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mAssistant> \u001b[0m\u001b[33mHere\u001b[0m\u001b[33m is a two-sentence poem about a\u001b[0m\u001b[33m llama:\n",
      "\n",
      "A llama's soft fur shines\u001b[0m\u001b[33m in the sun,\n",
      "Gently it walks\u001b[0m\u001b[33m, its day has\u001b[0m\u001b[33m just begun.\u001b[0m\u001b[97m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.1-405B-Instruct-FP8\"\n",
    "\n",
    "response = client.inference.chat_completion(\n",
    "    model_id=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a two-sentence poem about llama.\"},\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "import time\n",
    "\n",
    "# for x in response:\n",
    "#     print(x)\n",
    "for log in EventLogger().log(response):\n",
    "    log.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
