built_at: '2024-09-18T13:41:17.656743'
image_name: local
docker_image: null
conda_env: local
apis_to_serve:
- inference
- memory
- telemetry
- agents
- safety
- models
api_providers:
  inference:
    providers:
      - meta-reference
      - remote::ollama
  memory:
    providers:
      - meta-reference
      - remote::pgvector
  safety:
    providers:
      - meta-reference
  telemetry:
    provider_id: meta-reference
    config: {}
  agents:
    provider_id: meta-reference
    config:
      persistence_store:
        namespace: null
        type: sqlite
        db_path: /home/xiyan/.llama/runtime/kvstore.db
routing_table:
  inference:
    - routing_key: Meta-Llama3.1-8B-Instruct
      provider_id: meta-reference
      config:
        model: Meta-Llama3.1-8B-Instruct
        quantization: null
        torch_seed: null
        max_seq_len: 4096
        max_batch_size: 1
  memory:
    - routing_key: vector
      provider_id: meta-reference
      config: {}
  safety:
    - routing_key: llama_guard
      provider_id: meta-reference
      config:
        llama_guard_shield:
          model: Llama-Guard-3-8B
          excluded_categories: []
          disable_input_check: false
          disable_output_check: false
        prompt_guard_shield:
          model: Prompt-Guard-86M
    - routing_key: prompt_guard
      provider_id: meta-reference
      config:
        llama_guard_shield:
          model: Llama-Guard-3-8B
          excluded_categories: []
          disable_input_check: false
          disable_output_check: false
        prompt_guard_shield:
          model: Prompt-Guard-86M
    - routing_key: injection_shield
      provider_id: meta-reference
      config:
        llama_guard_shield:
          model: Llama-Guard-3-8B
          excluded_categories: []
          disable_input_check: false
          disable_output_check: false
        prompt_guard_shield:
          model: Prompt-Guard-86M
